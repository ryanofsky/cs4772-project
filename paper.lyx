#LyX 1.5.1 created this file. For more info see http://www.lyx.org/
\lyxformat 276
\begin_document
\begin_header
\textclass article
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\paperfontsize default
\spacing single
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title

\size large
Dimensionality Reduction for Visualization of Text Similarity
\end_layout

\begin_layout Author
Russell Yanofsky (rey4@columbia.edu)
\end_layout

\begin_layout Date
December 13, 2007
\end_layout

\begin_layout Abstract
Techniques which express high dimensional data in a reduced number of dimensions
 while maintaining a useful amount of the information content of the original
 data are useful in data processing and analysis.
 This paper explores a somewhat unlikely application of dimensionality reduction
 techniques, applying techniques extremely well suited to visualization
 of data in two or three dimensions to a collection of text documents in
 English, which comprise a dataset that is not normally subject to 2D plotting
 and spatial analysis.
 The techniques used are Semidefinite Embedding (SDE) and Minimum Volume
 Embedding (MVE), which are both based on the general technique of Kernel
 Principal Components Analysis (KPCA).
 A preprocessed representation of text based on word frequency (disregarding
 word order) is used as input to these algorithms.
 This type of representation has been shown in other work to be effective
 at text categorization with kernel methods, although in this case, the
 results for visualization do not turn out to be very compelling, which
 may be due to the broad specification of the problem, or the nature of
 the data set used in the experiment.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Textual data is among the most ubiquitous forms of data that exist today,
 and automated means of processing it, performing operations such as searching,
 categorization, translation, and correction, have had great success and
 are seeing constant improvements.
 But the sheer amount of text data means that even after text is automatically
 searched, categorized, translated, and corrected, navigating it may not
 be easy.
 To help manage this problem, visualization techniques have the potential
 to be applied to provide an aide to human navigation.
 The idea behind visualization for this purpose would be to show documents
 which are related to each other as points clustered together in a two or
 three dimensional space.
\end_layout

\begin_layout Standard
Visual representations like this could have uses in certain types of research,
 for example, in helping to sort through large archives of historical documents.
 It could also be used on the Internet, as an accompaniment to search results
 and the automated recommendation systems commonly seen on book and movie
 websites.
 Finally, it might have use in the testing and development of other natural
 language processing algorithms, providing a tool to summarize large amounts
 of data, and to help evaluate experimental results.
\end_layout

\begin_layout Standard
Different visualization techniques have been applied in the past to the
 problem of document visualization, sometimes using ad-hoc methods.
 For example, one system 
\begin_inset LatexCommand cite
key "allan97interactive"

\end_inset

 uses a feedback method, positioning points representing documents in a
 space, and iteratively moving the documents in space closer together and
 farther apart based on similarity scores.
 Another system 
\begin_inset LatexCommand cite
key "carey-info"

\end_inset

 uses a visualization technique called sammon mapping to compute the low
 dimensional representation.
\end_layout

\begin_layout Standard
This paper presents the application of two more recent visualization techniques,
 Semidefinite Embedding (SDE) 
\begin_inset LatexCommand cite
key "000sde"

\end_inset

, and Minimum Volume Embedding (MVE) 
\begin_inset LatexCommand cite
key "000bshaw"

\end_inset

 to documentation visualization, and shows the results of applying these
 techniques to a small data set.
\end_layout

\begin_layout Section
PCA Dimensionality Reduction Techniques
\end_layout

\begin_layout Standard
The dimensionality reduction techniques used here can be understood in the
 context of Principal Component Analysis (PCA).
 PCA is a widely used technique for dimensionality reduction.
 In its ideal scenario, a guassian (ellipsoid shaped) cloud of points is
 distributed around some mean point.
 Computing the covariance matrix of the points by 
\begin_inset Formula $C=\frac{1}{M}\sum_{j=1}^{M}\left(x_{j}-\overline{x}\right)\left(x_{j}-\overline{x}\right)^{\prime}$
\end_inset

 gives the maximum likelihood estimate of the guassian distribution's covariance
 parameter.
 Given this covariance parameter, the shape and orientation of the guassian
 distribution is known.
 In particular, the orientation of the guassian ellipsoid axes can be found
 by computing the eigenvectors of the covariance matrix.
 The longest axis, which points along the direction where the data points
 have the largest spread, is described by the eigenvector with the largest
 corresponding eigenvalue.
 The smallest axis 
\end_layout

\begin_layout Itemize
Given cloud of points centered around mean, wide in some directions, narrow
 in others
\end_layout

\begin_layout Itemize
Compute eignenvectors of covariance matrix, 
\begin_inset Formula $C=\frac{1}{M}\sum_{j=1}^{M}\left(x_{j}-\overline{x}\right)\left(x_{j}-\overline{x}\right)^{\prime}$
\end_inset


\end_layout

\begin_layout Itemize
Gives set of orthogonal axes through mean point, aligned with directions
 of greatest variance
\end_layout

\begin_layout Itemize
Best 2-D projection (capturing spread of data) is in plane of two axes with
 highest eigenvalues
\end_layout

\begin_layout Itemize

\size large
--
\end_layout

\begin_layout Itemize
PCA can be performed using dot products between points instead of point
 coordinates (Gram matrix instead of covariance matrix)
\end_layout

\begin_layout Itemize
Kernel functions between two points can be substituted for dot products
 allowing non-linear extension of PCA (for visualization, this means non-linear
 projections)
\end_layout

\begin_layout Itemize
Steps
\end_layout

\begin_deeper
\begin_layout Enumerate
Compute gram matrix 
\begin_inset Formula $K_{ij}=k(x_{i},x_{j})$
\end_inset


\end_layout

\begin_layout Enumerate
Center matrix 
\begin_inset Formula $\widetilde{K}_{ij}=K_{ij}-\frac{1}{M}\sum_{m=1}^{M}K_{mj}-\frac{1}{M}\sum_{n=1}^{M}K_{in}+\frac{1}{N^{2}}\sum_{m,n=1}^{M}K_{mn}$
\end_inset


\end_layout

\begin_layout Enumerate
Find eigenvectors and eigenvalues (eigenvectors 
\begin_inset Formula $\overrightarrow{\alpha_{n}}$
\end_inset

 normalized so that 
\begin_inset Formula $\lambda_{k}\left(\overrightarrow{\alpha_{k}}\cdot\overrightarrow{\alpha_{k}}\right)=1$
\end_inset

 for all 
\begin_inset Formula $k$
\end_inset

)
\end_layout

\begin_layout Enumerate
Take projection of points, 
\begin_inset Formula $V\Lambda$
\end_inset

, where 
\begin_inset Formula $V$
\end_inset

 is eigenvector matrix, 
\begin_inset Formula $\Lambda$
\end_inset

 is matrix with 
\begin_inset Formula $\sqrt{\lambda_{1}}\ldots\sqrt{\lambda_{M}}$
\end_inset

 along diagonal
\end_layout

\begin_layout Enumerate
Plot 2D visualization using 2 coordinates with highest eigenvalues, dropping
 other coordinates
\end_layout

\end_deeper
\begin_layout Standard
--
\end_layout

\begin_layout Itemize
Builds on Kernel PCA, finding optimal Kernel Matrix using semidefinite programmi
ng.
\end_layout

\begin_layout Itemize
Given a set of 
\begin_inset Quotes eld
\end_inset

neighbors
\begin_inset Quotes erd
\end_inset

 for each point, maintains distances between neighboring points while maximizing
 distances between unconstrained points.
\end_layout

\begin_layout Itemize
Advantage: If points are scattered on high dimensional manifold which twists
 and curves, fixing neighboring points and spreading out distant points
 
\begin_inset Quotes eld
\end_inset

flattens
\begin_inset Quotes erd
\end_inset

 manifold, giving good visualization of points lying on it.
\end_layout

\begin_layout Itemize
Program: Maximize 
\begin_inset Formula $tr\left(K\right)$
\end_inset

 (distance between points), subject to 
\begin_inset Formula $K\succ0$
\end_inset

 (positive semidefinite kernel matrix), 
\begin_inset Formula $\sum_{ij}K_{ij}=0$
\end_inset

 (centers kernel matrix), and 
\begin_inset Formula $K_{ii}+K_{jj}-K_{ij}-K_{ji}=G_{ii}+G_{jj}-G_{ij}-G_{ji}$
\end_inset

 where 
\begin_inset Formula $G_{ij}=x_{i}\cdot x_{j}$
\end_inset

 for all points 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $j$
\end_inset

 which are neighbor of each other or a common point (preserves local distances).
\end_layout

\begin_layout Itemize
--
\end_layout

\begin_layout Itemize
Builds on Semidefinite embedding.
 Instead of maximizing 
\begin_inset Formula $tr\left(K\right)=\sum_{i=1}^{N}\lambda_{i}$
\end_inset

, seeks to maximize 
\begin_inset Formula $\sum_{i=1}^{d}\lambda_{i}-\sum_{i=d+1}^{N}\lambda_{i}$
\end_inset


\end_layout

\begin_layout Itemize
Instead of maximizing distance between point in every dimension, only maximizes
 distance in the first 
\begin_inset Formula $d$
\end_inset

 dimensions being visualized, and minimize distance in remaining dimensions.
\end_layout

\begin_layout Itemize
This does a better job 
\begin_inset Quotes eld
\end_inset

flattening
\begin_inset Quotes erd
\end_inset

 the data, minimizing volume behind it, at cost of being more expensive
 to compute.
\end_layout

\begin_layout Itemize
Algorithm is iterative, taking existing kernel matrix, 
\begin_inset Formula $K$
\end_inset

, finding eigenvectors 
\begin_inset Formula $\overrightarrow{v_{i}}$
\end_inset

, performing modified SDE to minimize 
\begin_inset Formula $tr\left(K\left(-\sum_{i=1}^{d}\overrightarrow{v_{i}}\overrightarrow{v_{i}}{}^{\prime}+\sum_{i=d+1}^{N}\overrightarrow{v_{i}}\overrightarrow{v_{i}}{}^{\prime}\right)\right)$
\end_inset

, and repeating with new 
\begin_inset Formula $K$
\end_inset

 until convergence.
\end_layout

\begin_layout Section
String Kernels
\end_layout

\begin_layout Itemize
Each document is represented as a vector of word counts, normalized so documents
 of differing lengths can be compared.
\end_layout

\begin_layout Itemize

\emph on
Stemming
\emph default
 algorithm maps related words with different suffixes.
 For example, 
\begin_inset Quotes eld
\end_inset

computes
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

computing
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

computer
\begin_inset Quotes erd
\end_inset

, mapped into a canonical word stem form, 
\begin_inset Quotes eld
\end_inset

comput
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Itemize
To cut down on irrelevant features, a stop word list is used to remove words
 like 
\begin_inset Quotes eld
\end_inset

and
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

or
\begin_inset Quotes erd
\end_inset

, and 
\begin_inset Quotes eld
\end_inset

the
\begin_inset Quotes erd
\end_inset

 from feature vectors.
\end_layout

\begin_layout Itemize
Remaining words are weighted by 
\emph on
inverse document frequency
\emph default
 (IDF), which is just one over the total number of documents a word appears
 in.
\end_layout

\begin_layout Itemize
RBF kernel used with above preprocessing steps, since this kernel and representa
tion have been shown to be effective for text categorization (Joachims 98)
\end_layout

\begin_layout Section
Experiment
\end_layout

\begin_layout Itemize
Experiment was to do two dimensional visualization with issue documents
 from 2008 presidential candidates' campaign web sites.
\end_layout

\begin_layout Itemize
Each document was labeled for the its topic (environment, healthcare, foreign
 policy, etc.), and the candidate whose views it expressed (Clinton, Giuliani,
 etc.)
\end_layout

\begin_layout Itemize
Having dataset with two distinct labels for each point, makes visualization
 more interesting, makes it easier to look for patterns in output.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Itemize
In progress...
\end_layout

\begin_layout Itemize
So far no clear or especially meaningful patterns in visualizations have
 emerged
\end_layout

\begin_layout Itemize
Sample visualization: Kernel PCA with Gaussian kernel, each candidate a
 different color, each topic a different symbol
\end_layout

\begin_layout Standard
\begin_inset LatexCommand bibtex
options "plain"
bibfiles "refs"

\end_inset


\end_layout

\end_body
\end_document
